{"Notes/Linear-Algebra/Best-Approximation":{"title":"Best Approximation","links":["Notes/Linear-Algebra/Orthogonal-Decomposition"],"tags":[],"content":"W is a subspace of \\mathbb{R}^n, \\vec{y} \\in \\mathbb{R}^n, \\hat{y} = proj_{W} \\vec{y}\n\\implies\n\\forall \\vec{w} \\not = \\hat{y} \\in W \\quad \\quad ||\\vec{y} - \\hat{y}|| &lt; ||\\vec{y} - \\vec{w}||\nIn other words, \\hat{y} is closest vector in W to \\vec{y}\nSee Orthogonal Decomposition, especially for the visual interpretation of this."},"Notes/Linear-Algebra/Constrained-Optimization":{"title":"Constrained Optimization","links":[],"tags":[],"content":"Q: When x^T \\cdot u = 0 for the first greatest eigenvalue, and the remaining ones were -2, and 0, the next maximum was -2 instead of 0?"},"Notes/Linear-Algebra/Diagonalizable":{"title":"Diagonalizable","links":["tags/todo"],"tags":["todo"],"content":"A \\in \\mathbb{R}^{n \\times n} \\land A = PDP^{-1} \\implies A is diagonalizable, where D is a diagonal matrix\nA is diagonalizable \\iff A has n linearly independent eigenvectors.\ni.e.\n\\displaylines{\nA = PDP^{-1}\\\\ \\iff \\\\ A = [\\vec{v_1}, ..., \\vec{v_n}] \n\\begin{bmatrix}\n\\lambda_1 &amp;  &amp;  \\\\\n &amp; ... &amp;  \\\\\n &amp;  &amp; \\lambda_n\n\\end{bmatrix}\n[\\vec{v_1}, ..., \\vec{v_n}]^{-1}\n}\nwhere \\vec{v} vectors are linearly independent eigenvectors, and \\lambda_1, ..., \\lambda_n are the corresponding eigenvalues, in order.\nDistinct Eigenvalues\nIf A \\in \\mathbb{R}^{n \\times n} and has n distinct eigenvalues, then A is diagonalizable\nNon-distinct Eigenvalues\nYou check that the sum of the geometric multiplicities is equal to the size of the matrix.\ne.g. for\n\\begin{bmatrix}\n1 &amp;  &amp; -1 \\\\\n &amp; 2 &amp;  \\\\\n &amp;  &amp; 1\n\\end{bmatrix}\nFind the eigenvalues:\n\\displaylines{\n\\begin{vmatrix}\n1-\\lambda &amp;  &amp; -1 \\\\\n &amp; 2-\\lambda &amp;  \\\\\n &amp;  &amp; 1-\\lambda\n\\end{vmatrix} = (1-\\lambda)^2 (2-\\lambda) = 0\\\\\n\\therefore \\lambda = 1,1,2\n}\nWe know that geomult ⇐ algmult. Therefore \\lambda = 2 has 1 distinct eigenvector.\nThis means \\lambda = 1 has to have 2 distinct eigenvectors to form a basis, so if it doesn’t then the matrix is not diagonalizable.\nA - I = \\begin{bmatrix}\n0 &amp;  &amp; -1 \\\\\n &amp; 1 &amp;  \\\\\n &amp;  &amp; 0\n\\end{bmatrix}\nThere is only one free columns here. Therefore, the dimension of the Nullspace is one, not two, which means the matrix is not diagonalizable.\nBasis of Eigenvectors\n\\displaylines{\n\\text{Express the vector $\\vec{x}_0 =$}\n\\begin{bmatrix}\n4 \\\\\n5\n\\end{bmatrix}\n\\text{ as a linear combination of the vectors }\\\\\n\\vec{v}_1 = \\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\n\\text{ and }\n\\vec{v_2} =\\begin{bmatrix}\n1 \\\\\n-1\n\\end{bmatrix}\n\\text{ and find the coordinates of } \\vec{x_0} \\text{ in the basis}\\\\\n\\mathcal{B} = \\{\\vec{v_1}, \\vec{v_2}\\}\\\\\n\\\\\n[\\vec{x_0}]_{\\mathcal{B}} = ?\\quad\\quad\\quad\n[\\vec{x_0}]_{\\mathcal{B}} = \\begin{bmatrix}\n4.5 \\\\\n-0.5\n\\end{bmatrix}\\\\\n\\\\\n\\sim \\\\\n\\\\\n\\text{Let } P = [\\vec{v_1}\\ \\vec{v_2}],\\ D = \\begin{bmatrix}\n1 &amp; 0 \\\\\n0 &amp; -1\n\\end{bmatrix}\\\\\n\\\\\n[A^k\\ \\vec{x}_0]_{\\mathcal{B}} =\\ ? \\quad\\quad\\text{where } A = PDP^{-1},\\ k\\in \\mathbb{Z}^{+}\\\\\n\\\\\nA^k = PD^k P^{-1} = [\\vec{v_1}\\ \\vec{v_2}]\n\\begin{bmatrix}\n 1^k &amp;  \\\\\n &amp; (-1)^k\n\\end{bmatrix}  \n[\\vec{v_1}\\ \\vec{v_2}]^{-1}\\\\\n[A^k\\ \\vec{x}_0]_{\\mathcal{B}} =\\ ?\n}\ntodo\nMisc.\n\\displaylines{\n\\text{Let d(x) be &quot;x is diagonalizable&quot;}\\\\\n\\text{Let i(x) be &quot;x is invertible&quot;}\\\\\nd(A) \\land i(A) \\iff d(A^{-1})\\land i(A^{-1})\n}"},"Notes/Linear-Algebra/Diagonalization":{"title":"Diagonalization","links":["Notes/Linear-Algebra/Diagonalizable","Notes/Linear-Algebra/Orthogonal-Diagonalization"],"tags":[],"content":"Diagonal Matrix\nA matrix is diagonal if the only non-zero elements, if any, are on the main diagonal.\n\nIf A is diagonal, then A^k is very easy to compute because you simply exponentiate every diagonal element by k\n\nDiagonal matrices cannot have \\lambda = 0\nStrang\nConsider a matrix A with some eigenvalues and eigenvectors.\nA\\vec{v_0} = \\lambda_0 \\vec{v_0}\n…\nA\\vec{v_n} = \\lambda_n \\vec{v_n}\n\\displaylines{\nA\\ [\\vec{v_0}\\ ,\\ ...\\ ,\\ \\vec{v_n}] = [\\lambda_0 \\vec{v_0}\\ ,\\ ...\\ ,\\ \\lambda_n \\vec{v_n}]\\\\\n\nA\\ [\\vec{v_0}\\ ,\\ ...\\ ,\\ \\vec{v_n}]= [\\vec{v_0}\\ ,\\ ...\\ , \\vec{v_n}]\\begin{bmatrix}\n\\lambda_0 &amp;  &amp;  \\\\\n &amp; ... &amp;  \\\\\n &amp;  &amp; \\lambda_n\n\\end{bmatrix}\\\\\n\nA\\ = [\\vec{v_0}\\ ,\\ ...\\ , \\vec{v_n}]\\begin{bmatrix}\n\\lambda_0 &amp;  &amp;  \\\\\n &amp; ... &amp;  \\\\\n &amp;  &amp; \\lambda_n\n\\end{bmatrix} [\\vec{v_0}\\ ,\\ ...\\ ,\\ \\vec{v_n}]^{-1}\n}\nDiagonalizable\nOrthogonal Diagonalization"},"Notes/Linear-Algebra/Google-Page-Rank":{"title":"Google Page Rank","links":[],"tags":[],"content":""},"Notes/Linear-Algebra/Gram-Schmidt-Process":{"title":"Gram-Schmidt Process","links":["Notes/Linear-Algebra/Orthogonal-Basis","Notes/Linear-Algebra/Orthogonal-Decomposition"],"tags":[],"content":"Let X be a set of vectors that form a basis for subspace W of \\mathbb{R}^n,\nLet V be a set of vectors that form an Orthogonal Basis for W\nThe Gram-Schmidt process defines how V can be derived from X\nIt depends on Orthogonal Decomposition\nIf X = \\{\\vec{x_1}, \\cdots \\vec{x_p}\\}, iteratively define vectors in V = \\{\\vec{v_1}, \\cdots, \\vec{v}_p\\}\n\\vec{v_1}=\\vec{x_1}\n\\vec{v_2}=\\vec{x_2} - proj_{\\vec{v_1}}\\vec{x_2}\n\\vec{v_3}=\\vec{x_3} - proj_{\\vec{v_1}}\\vec{x_3} -  proj_{\\vec{v_2}}\\vec{x_3}\n\\cdots\n\\vec{v}_p = \\vec{x_p} - proj_{\\vec{v}_1} \\vec{x_p} - \\cdots - proj_{\\vec{v}_{p-1}} \\vec{x}_{p}\n\\vec{v}_p = \\vec{x_p} - \\sum_{i = 1}^{p - 1} \\text{proj}_{\\vec{v}_i}\\vec{x}_p\n\nExamples\nHW 6.4 Q1\nHW 6.4 Q2"},"Notes/Linear-Algebra/Least-Squares":{"title":"Least Squares","links":["Notes/Linear-Algebra/Best-Approximation","Notes/Linear-Algebra/Orthogonal-Decomposition","LeastSquaresHW6_5.pdf"],"tags":[],"content":"Given\nGiven many data points, construct a matrix equation in the form of a linear equation (this matrix equation will be overdetermined). The matrix equation below is A\\vec{x} = \\vec{b}\nThis equation is linear but it doesn’t have to be, just adjust accordingly to represent the equations as a matrix equation.\n\\begin{bmatrix}\n1 &amp; x_0 \\\\\n1 &amp; x_1 \\\\\n1 &amp; x_2 \\\\\n1 &amp; x_3 \\\\\n1 &amp; x_4 \\\\\n1 &amp; x_5\n\\end{bmatrix}\\begin{bmatrix}\nb \\\\\nm\n\\end{bmatrix} = \\begin{bmatrix}\ny_0 \\\\\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\ny_4 \\\\\ny_5\n\\end{bmatrix}\nGoal\nUsing Best Approximation, find a vector in subspace Col\\ A closest to \\vec{b}\nCol\\ A is a subspace of \\mathbb{R}^n, \\vec{b} \\in \\mathbb{R}^n\n\\forall \\vec{x},\\ \\vec{a} = A\\vec{x}, i.e. \\vec{a} \\in Col\\ A\n\\hat{b} = A\\hat{x} = proj_{Col\\ A} \\vec{b}\nNote: Can only use Orthogonal Decomposition for \\hat{b} when the columns of A form an orthogonal basis, by definition\n\\implies\n\\displaylines{\n\\forall \\vec{a}\\in Col\\ A,\\ \\vec{a} \\not = \\hat{b} \\quad \\quad ||\\vec{b} - \\hat{b}|| &lt; ||\\vec{b} - \\vec{a}||\\\\\n\\forall A\\vec{x} \\in Col\\ A,\\ A\\vec{x} \\not = A\\hat{x} \\quad \\quad ||\\vec{b} - A\\hat{x} || &lt; ||\\vec{b} - A\\vec{x}||\\\\\n}\nIn other words, \\hat{b} = A\\hat{x} = proj_{col\\ a} \\vec{b} is closest vector in Col\\ A to \\vec{b}\nNote: \\hat{x} is a unique vector, a special \\vec{x} that minimizes the above equation.\nNote: If the columns of A are orthogonal, then you can just use the scalar projection of \\vec{b} onto each column of A.\n\nNormal Equations\nThe least squares solutions to A\\vec{x} = \\vec{b} corresponds to the solution to\nA^T A\\vec{x} = A^T \\vec{b}\n\nTurns the A\\vec{x} = \\vec{b} equation from above and transforms it into a square matrix equation\n\nDerivation\n\n\n(Col A)^{\\perp} = Null(A^T)\n\\hat{x} is the Least Squares Solution to A\\vec{x} = \\vec{b} \\iff b - A\\hat{x} \\perp Col\\ A\n\\vec{b} - A\\hat{x} \\perp Col\\ A \\implies \\vec{b} - A\\hat{x} \\in (Col\\ A)^{\\perp}\n\\vec{b} - A\\hat{x} \\in (Col\\ A)^{\\perp} \\implies \\vec{b} - A\\hat{x} \\in Nul\\ A^T\n\\vec{b} - A\\hat{x} \\in Nul\\ A^T \\iff A^T (\\vec{b} - A\\hat{x}) = 0\nA^T (b - A\\hat{x}) = 0\nA^T b - A^T A\\hat{x} = 0\nA^T A\\hat{x} = A^T \\vec{b}\n\nNormal Equation Usage\n\nUse when non-square matrix\n\nOver/Under determined\n\n\nRegression\n\nTheorem (Unique Solutions for Least Squares)\nIf A is m x n\n\nAx = b has a unique least squares solution for each b in Rm\nCols of A are linearly independent\nThe matrix A^T A is invertible\nIf the above hold, the unique least square solution is\n\n\\hat{x} = (A^T A)^{-1} A^T \\vec{b}\nIf the above conditions are not true, there may be infinitely many solutions, or some other nonunique amount of solutions, in which case you should consider [A^T A\\ A^T \\vec{b}] instead.\nNote: A^T A plays the role of the “length squared” of the matrix A\nTheorem (Least Squares and QR)\n\\displaylines{\nA \\in \\mathbb{R}^{m \\times n} = QR\\\\\n\\implies\\\\\n\\text{Least Squares Solution }\n\\hat{x} = R^{-1} Q^T \\vec{b}\n}\nExamples\n\\displaylines{\nA = \\begin{bmatrix}\n4 &amp; 0 \\\\\n 0&amp;2  \\\\\n1 &amp; 1\n\\end{bmatrix}, \\quad \\vec{b} = \\begin{bmatrix}\n2 \\\\\n0 \\\\\n11\n\\end{bmatrix}\\\\\n\\\\\nA^T A = \\begin{bmatrix}\n17 &amp; 1 \\\\\n1 &amp; 8\n\\end{bmatrix}\\\\\nA^T \\vec{b} = \\begin{bmatrix}\n19 \\\\\n11\n\\end{bmatrix}\\\\\n\\text{Now setup the Normal Equations:}\\\\\nA^T A \\vec{x}= A^T \\vec{b}\\\\\n\n\\begin{bmatrix}\n17 &amp; 1 \\\\\n1 &amp; 8\n\\end{bmatrix}\\vec{x} = \\begin{bmatrix}\n19 \\\\\n11\n\\end{bmatrix}\\\\\n\n\\vec{x} = \\begin{bmatrix}\n17 &amp; 1 \\\\\n1 &amp; 8\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n19 \\\\\n11\n\\end{bmatrix}\\\\\n\n\\hat{x} = \n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n}\nSee also: LeastSquaresHW6_5.pdf\nHampton Explanation for Least Squares\nLet A \\in \\mathbb{R}^{m \\times n}. \\hat{x} is the unique, minimizing solution to the equation A\\vec{x} = \\vec{b} such that\n\\forall \\vec{x} \\in \\mathbb{R}^n \\quad\\quad ||\\vec{b} - A\\hat{x}|| \\leq ||\\vec{b} - A \\vec{x}||\n\nEssentially, minimize \\vec{b} - A\\hat{x}\n\n||\\vec{b} - \\hat{b}|| is the minimal distance between the different solutions\n\n\n\\forall x \\in \\mathbb{R}^n,\\ A\\vec{x} \\in Col\\ A,\\ \\vec{b} =^{?} Col\\ A\nGoal: Find \\hat{x} s.t. A\\hat{x} \\in Col\\ A is closest to \\vec{b}\n\\hat{x} in this context just denotes the special/unique x that minimizes the distances between \\vec{b} and A\\hat{x}\n\nb is closer to Axhat than to Ax for all other x in Col A\nIf b in Col A, then xhat is…\nSeek \\hat{x} so that A\\hat{x} is as close to \\vec{b} as possible, i.e. \\hat{x} should solve Axhat = bhat\n\n\\hat{b} = A\\hat{x}= proj_{Col(A)} \\vec{b}\n\n\n"},"Notes/Linear-Algebra/Master":{"title":"Master","links":["tags/star","Notes/Linear-Algebra/Diagonalization","Notes/Linear-Algebra/Diagonalizable","Notes/Linear-Algebra/Orthogonal-Complement","Notes/Linear-Algebra/Orthogonal-Projection","Notes/Linear-Algebra/Orthogonal-Decomposition","Notes/Linear-Algebra/Best-Approximation","Notes/Linear-Algebra/Gram-Schmidt-Process","Notes/Linear-Algebra/QR-Factorization","Notes/Linear-Algebra/Least-Squares","Notes/Linear-Algebra/Google-Page-Rank","Notes/Linear-Algebra/Symmetric-Matrix","Notes/Linear-Algebra/Spectral-Decomposition","Notes/Linear-Algebra/Orthogonal-Diagonalization","Notes/Linear-Algebra/Quadratic-Form","Notes/Linear-Algebra/Constrained-Optimization","Notes/Linear-Algebra/Singular-Value-Decomposition"],"tags":["star"],"content":"Barone Website\nMaster Website\nSystems of Linear Equations\nsbarone7.math.gatech.edu/Chapters_1_and_2.pdf\nLinear Equation\na_1x_1 + a_2x_2 + ... + a_n x_n = b\na’s are coefficients, x’s are variables, n is the dimension (number of variables)\ne.g. 2x_1 + 4x_2 = 4 has two dimensions\nRow Reduction\nRow Operations\n\nReplacement/Addition\nInterchange\nScaling\nRow operations can be used to solve systems of linear equations\n\n\nA system of equations written as an augmented matrix\nRow operation example (these are augmented)\n\\begin{bmatrix}\n1 &amp; -2 &amp; 1 &amp; 0 \\\\\n0 &amp; 2 &amp; -8 &amp; 8 \\\\\n5 &amp; 0 &amp; -5 &amp; 0\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; -2 &amp; 1 &amp; 0 \\\\\n0 &amp; 2 &amp; -8 &amp; 8 \\\\\n0 &amp; -10 &amp; 10 &amp; 10\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; -2 &amp; 1 &amp; 0 \\\\\n0 &amp; 2 &amp; -8 &amp; 8 \\\\\n0 &amp; -10 &amp; 10 &amp; 10\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; -2 &amp; 1 &amp; 0 \\\\\n0 &amp; 2 &amp; -8 &amp; 8 \\\\\n0 &amp; 0 &amp; 30 &amp; -30\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; -2 &amp; 1 &amp; 0 \\\\\n0 &amp; 1 &amp; -4 &amp; 4 \\\\\n0 &amp; 0 &amp; 1 &amp; -1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; -2 &amp; 1 &amp; 0 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 1 &amp; -1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; 0 &amp; 0 &amp; 1 \\\\\n0 &amp; 1 &amp; 0 &amp; 0 \\\\\n0 &amp; 0 &amp; 1 &amp; -1\n\\end{bmatrix}\nA linear system is considered consistent if it has &gt;=1 solution(s)\nIf two matrices are row equivalent they have the same solution set, meaning that one can be transformed into the other\nRow Reduction and Echelon Forms\nA rectangular matrix is in echelon form if\n\nIf there are any, all zero rows are at the bottom\nThe first non-zero entry (leading entry) of a row is to the right of any leading entries in the row above it\nAll elements below a leading entry are zero\n\nFor reduced row echelon form\nAll leading entries, if any, are equal to 1.\nLeading entries are the only nonzero entry in their respective column.\n\nPivots and Free Variables\nPivot position in a matrix A is a location in A that corresponds to a leading 1 in the REF of A\nPivot column is the column of the pivot\nFree variables are the variables of the non-pivot columns\nAny choice of the free variables leads to a solution of the system\n^[If you have any free variables you do not have a unique solution]\n\nTheorem for Consistency\n\nA linear system is consistent iff the last\ncolumn of the augmented matrix does not have a pivot. This is\nthe same as saying that the RREF of the augmented matrix does\nnot have a row of the form [0\\ 0\\ 0\\ 0\\ ...\\ |\\ 1]\nMoreover, if a linear system is consistent, then it has\n1. a unique solution iff there are no free variables.\n2. infinitely many solutions that are parameterized by free variables\n\nVector Equations\n\\mathbb{R} is all real numbers\n\\mathbb{R}^n is n dimensions of \\mathbb{R}\n\\mathbb{R}^{n \\times m} is n rows and m columns\nLinear Combination\nLet c_i \\in \\mathbb{R} \\land \\vec{v}_i \\in \\mathbb{R}^{&gt;=1}\nc_1 \\vec{v}_1 + ... c_n \\vec{v}_n = \\vec{B}\nis a linear combination of the v vectors, with weights of the c‘s\nSpan\n\nThe set of all linear combinations of the v‘s in called the span of the v‘s\ne.g.\n\nSPAN(\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix}, \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}) = \\mathbb{R}^2\n\nany 2 vectors in \\mathbb{R}^2 that are not scalar multiplies of each other span \\mathbb{R}^2\nQ: Is \\vec{b} \\in SPAN(\\vec{a}_1, \\vec{a}_2)\n\n\\vec{b} = \\begin{bmatrix}\n7 \\\\\n4 \\\\\n3\n\\end{bmatrix}, \\vec{a}_1 = \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n-5\n\\end{bmatrix}, \\vec{a}_2 = \\begin{bmatrix}\n2 \\\\\n5 \\\\\n6\n\\end{bmatrix}\nMatrix below in form of system of equations where X and Y scale columns 0 and 1, and column 2 are coefficients on the right hand side of the equation. By reducing this matrix to RREF, we can systematically reveal the values of X and Y\n\\begin{bmatrix}\n1 &amp;  2&amp;  7\\\\\n-2 &amp; 5&amp;  4\\\\\n-5 &amp;  6&amp;  3\n\\end{bmatrix}\n\n\\begin{bmatrix}\n1 &amp;  2&amp;  7\\\\\n0 &amp; 9&amp;  18\\\\\n-5 &amp;  6&amp;  3\n\\end{bmatrix}\n\n\\begin{bmatrix}\n1 &amp;  2&amp;  7\\\\\n0 &amp; 1&amp;  2\\\\\n-5 &amp;  6&amp;  3\n\\end{bmatrix}\n\n\\begin{bmatrix}\n1 &amp;  2&amp;  7\\\\\n0 &amp; 1&amp;  2\\\\\n0 &amp;  16&amp;  38\n\\end{bmatrix}\n\n\\begin{bmatrix}\n1 &amp;  2&amp;  7\\\\\n0 &amp; 1&amp;  2\\\\\n0 &amp;  0&amp;  0\n\\end{bmatrix}\n\n\\begin{bmatrix}\n1 &amp;  0&amp;  3\\\\\n0 &amp; 1&amp;  2\\\\\n0 &amp;  0&amp;  0\n\\end{bmatrix}\nYes.\n\\vec{A} \\vec{x} = \\vec{b} exists\n\n\\begin{bmatrix}\n2 &amp; 3 &amp; 7 \\\\\n1 &amp; -1 &amp; 5\n\\end{bmatrix}\n\n\\begin{bmatrix}\n5 &amp; 0 &amp; 22 \\\\\n1 &amp; -1 &amp; 5\n\\end{bmatrix}\n\n\\begin{bmatrix}\n1 &amp; 0 &amp; 22/5 \\\\\n0 &amp; 1 &amp; 22/5-5\n\\end{bmatrix}\nHomogeneous vs Inhomogeneous\nHomogeneous:\n\\vec{A} \\vec{x} = \\vec{0}\nHomogeneous systems always have a trivial \\vec{0} solution, so naturally we would like to know if there are nontrivial, perhaps infinite solutions, namely if there is a free variable and a column with no pivots\nParametric vector forms of solutions to linear systems\nYou can parameterize the free variables and then write the solution as a vector sum.\nThe solution is \\vec{x} which is\nx_1 = 2x_3, x_2 = -x_3, x_3=x_3\nLet x_3 = t\n\\vec{x} = \\begin{bmatrix}\n2 \\\\\n-1 \\\\\n1\n\\end{bmatrix} t\nNonhomogenous System\nBecause right side of augmented is nonzero:\n\\begin{bmatrix}\n1 &amp; 3 &amp; 1 &amp; 9 \\\\\n2 &amp; -1 &amp; -5 &amp; 11 \\\\\n0 &amp; 1 &amp; -2 &amp; 6\n\\end{bmatrix} \\rightarrow^{RREF} \\begin{bmatrix}\n1 &amp; 0 &amp; -2 &amp; 6 \\\\\n0 &amp; 1 &amp; 1 &amp; 1 \\\\\n0 &amp; 0 &amp; 0 &amp; 0\n\\end{bmatrix}\nLet x_3 = t\n\\vec{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\\\\n-1 \\\\\n1\n\\end{bmatrix} t\n+ \\begin{bmatrix}\n6 \\\\\n1 \\\\\n0\n\\end{bmatrix}\nLinear Independence\nGiven \\vec{A} \\vec{x} = \\vec{0}, if the only solution \\vec{x} is \\vec{0} \\implies Linearly Independent\nNote: (This might just be wrong)\n\\vec{A} = [\\vec{a_0}\\ ... \\vec{a_n}]\\ \\big{|}\\ \\vec{a_i} \\in \\mathbb{R}^n\n\\land\nspan([\\vec{a_0}\\ ... \\vec{a_n}]) = \\mathbb{R}^n \\quad \\quad ^[Same as having n rows of pivots]\n\\iff Linearly Independent\nDependence\n\nAny of the vectors in the set are a linear combination of the others\nIf there is a free variable, so there are infinite solutions to the homogenous equation\nIf the columns of A are in \\mathbb{R}^n, and there are n basis vectors in \\mathbb{R}^n (which is always true), then if the amount of columns in A exceeds the amount of basis vectors that exist in that dimension, it means that there are free variables, which indicates linear dependence\nIf one or more of the columns of A is \\vec{0}\n\nGeometric interpretation of linearly independent vectors\nIf two vectors are linearly independent, the are not colinear\nIf 3, then not coplanal\nIf 4, not cospacial\nIntro to Linear Transformations\nLinear Transformation\nA \\in R^{m \\times n}\nLinear transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m, T(\\vec{v}) = A \\vec{v}\n\nDomain of T is \\mathbb{R}^n (where we start)\nCodomain or target of T is \\mathbb{R}^m\nThe vector T(\\vec{x}) is the image of \\vec{x} under T\nThe set of all possible images is called the range\nimage \\in range \\in codomain\nWhen the domain and codomain are both \\mathbb{R}, you can represent them as a Cartesian Graph in \\mathbb{R}^2, as in a mapping of \\mathbb{R} \\rightarrow \\mathbb{R}\n\nIf y is the codomain and x is the domain, the range is the range, the domain is all the images of f(x)\n\n\n\nThe interpretation of matrix multiplication as a linear transformation\nA = \\begin{bmatrix}\n1 &amp; 1 \\\\\n0 &amp; 1 \\\\\n1 &amp; 1\n\\end{bmatrix}, \\vec{u} = \\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}, \\vec{b} = \\begin{bmatrix}\n7 \\\\\n5 \\\\\n7\n\\end{bmatrix}\nT: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\nCompute T(\\vec{u})\nA \\vec{u} = \\begin{bmatrix}\n1 &amp; 1 \\\\\n0 &amp; 1 \\\\\n1 &amp; 1\n\\end{bmatrix}\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix} = \\begin{bmatrix}\n7 \\\\\n4 \\\\\n7\n\\end{bmatrix}\nCalculate \\vec{v} \\in \\mathbb{R}^2 so that T(\\vec{v}) = \\vec{b}\nA\\vec{v} = \\vec{b}\n\\begin{bmatrix}\n1 &amp; 1 &amp; 7\\\\\n0 &amp; 1 &amp; 5\\\\\n1 &amp; 1 &amp; 7\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; 0 &amp; 2\\\\\n0 &amp; 1 &amp; 5\\\\\n1 &amp; 1 &amp; 7\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; 0 &amp; 2\\\\\n0 &amp; 1 &amp; 5\\\\\n0 &amp; 1 &amp; 5\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; 0 &amp; 2\\\\\n0 &amp; 1 &amp; 5\\\\\n0 &amp; 0 &amp; 0\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &amp; 0 &amp; 2\\\\\n0 &amp; 1 &amp; 5\\\\\n\\end{bmatrix}\n\\vec{v} = \\begin{bmatrix}\n2 \\\\\n5\n\\end{bmatrix}\nGive a\n\\vec{c} \\in \\mathbb{R}^3 | \\neg \\exists \\vec{v} | T(\\vec{v}) = \\vec{c}\n\\lor\nGive a \\vec{c} that is not in the range of T\n\\lor\nGive a \\vec{c} that is not in the span of the columns of A\n(Same question for all)\nRange of T is a bunch of images of the following form:\nA \\vec{v} = \\begin{bmatrix}\nv_1 + v_2 \\\\\nv_2 \\\\\nv_1 + v_2\n\\end{bmatrix}\nFor \\vec{c} to not be in the range of T, it cannot be in the above form, e.g. it can be\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\nLinear\nA function T: \\mathbb{R}^n \\rightarrow m is linear if\n\nT(u + v) = T(u) + T(v)\nT(c\\vec{v}) = cT(v)\n“Principle of Superposition”\n\nIf we know T(e_1), …,  T(e_n) then we know every T(v)\n\n\nProve it is linear by proving the addition and multiplication rules\n\nMatrix of Linear Transformations\nThe standard (basis) vectors\nStandard vectors in \\mathbb{R}^n have a one entry for each dimension, and zeros for the rest,\ne.g. for \\mathbb{R}^3:  \\vec{e_1}, \\vec{e_2}, \\vec{e_3} = \\hat{i}, \\hat{j}, \\hat{k}\nStandard matrix\nTheorem\n\nLet T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m be a linear transformation. Then there is a unique matrix A such that\nT(\\vec{x}) = A\\vec{x}, \\vec{x} \\in \\mathbb{R}^n\nIn fact, A is a m \\times n and its j^{th} column is a vector T(\\vec{e_j})\nA = [T(\\vec{e_1}), T(\\vec{e_2}), ... T(\\vec{e_n})]\n\nTwo and three dimensional transformations in more detail.\nT(\\vec{e_1}) = \\begin{bmatrix}\n5 \\\\\n -7\\\\\n2\n\\end{bmatrix}, T(\\vec{e_2})=\\begin{bmatrix}\n-3 \\\\\n -8\\\\\n0\n\\end{bmatrix}\nA = \\begin{bmatrix}\nT(\\vec{e_1}) &amp; T(\\vec{e_2}) \n\\end{bmatrix} = \\begin{bmatrix}\n5 &amp; -3 \\\\\n-7 &amp;  -8\\\\\n2 &amp; 0\n\\end{bmatrix}\nFind standard matrix A for T(x) = 3\\vec{x} for x in \\mathbb{R}^2\nA \\in \\mathbb{R}^2\nA = \\begin{bmatrix}\nT(\\vec{e_1}) &amp; T(\\vec{e_2}) \n\\end{bmatrix} = \\begin{bmatrix}\n\\begin{bmatrix}\n3 \\\\\n0\n\\end{bmatrix} &amp; \\begin{bmatrix}\n0 \\\\\n3\n\\end{bmatrix}\n\\end{bmatrix} = \\begin{bmatrix}\n3 &amp; 0 \\\\\n0 &amp; 3\n\\end{bmatrix}\nOnto and one-to-one transformations\nTLDR:\n1-1 \\iff every column of T is a pivot column\nOnto \\iff every row of T is a pivot row\nOnto\nA linear transformation T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m is onto if there exists a location in the codomain for every location in the domain\nonto iff the standard matrix has a pivot in every row\nThe matrix A has columns which span \\mathbb{R}^m.\nThe matrix A has all pivotal rows.\n1-1\n\nIf there is at most one location in the codomain for every location in the domain\n1-1 iff standard matrix has pivot in every column\n\nExample(s)\n\ne.g. F(x) = x^2 is not 1-1, because multiple x values for a single y value\n\nThe unique solution to T (\\vec{x}) = \\vec{0} is the trivial one.\nThe matrix A linearly independent columns.\nEach column of A is pivotal.\n1-1 and Onto\nneed square matrix\nif 1-1 then onto\nif Onto then 1-1\nIdentity and zero matrices\n0 Matrix is matrix full of zeroes\nIdentity matrix is a square matrix full of zeroes except for the diagonal, which is all ones. Multiplying with identity matrix always yields the same matrix.\nMatrix algebra (sums and products, scalar multiplies, matrix powers)\nSums: same dimensions\nMatrix multiplication: r_1 * (c_1 \\times r_2) * c_2 \\rightarrow r_1 \\times c_2, AB \\neq BA, AB = AC \\not \\implies B = C, AB = 0 \\not \\implies A = 0 \\lor B = 0, AB = [Ab_1\\ Ab_2]\nTranspose of a matrix\nA=\\begin{bmatrix}\na &amp; b \\\\\nc &amp; d \\\\\ne &amp; f\n\\end{bmatrix}, A^T = \\begin{bmatrix}\na &amp; b &amp; c \\\\\nd &amp; e &amp; f\n\\end{bmatrix}\nTranspose Properties\n(A^T)^T = A\n(A + B)^T + A^T + B^T\n(sA)^T = s(A^T)\n(AB)^T = B^T A^T\nInvertibility\nDefinition:\nA \\in \\mathbb{R}^{n \\times n} is invertible if \\exists C \\in \\mathbb{R}^{n \\times n} s.t. AC = CA = I_n\nA is invertible \\implies A is square\nA is invertible \\iff it is row equivalent to the identity\nstar\nLinearly dependent \\iff Singular\n\nMnemonic: After the trial, Johnny Depp was Single\nLinearly independent \\iff Invertible\nThis is just the inverse of the above\n\nalso\n\nA \\in \\mathbb{R}^{n \\times n} is invertible \\iff \\forall \\vec{b}\\ \\exists ! \\vec{x}\\ (A \\vec{x} = \\vec{b})\n\nBasically means that A is 1-1 and Onto, meaning that there is exactly one domain entry for every codomain entry\n\n(1-1 is at most 1, Onto is at least 1, together they make exactly 1)\n\n\n\n\ndet(A) \\not = 0 \\iff invertible\n\nInverse Properties\n(A^{-1})^{-1} = A\n(AB)^{-1} = B^{-1} A^{-1}\n(A^T)^{-1} = (A^{-1})^T\n2 \\times 2 Inverse Shortcut\nA = \\begin{bmatrix}\na &amp; b \\\\\nc &amp; d\n\\end{bmatrix}\n\\begin{bmatrix}\na &amp; b \\\\\nc &amp; d\n\\end{bmatrix}^{-1} = \\frac{1}{det(A)}\n\\begin{bmatrix}\nd &amp; -b \\\\\n-c &amp; a\n\\end{bmatrix}\nElementary Matrix\nAn elementary matrix, E, is one that differs by I_n by one row operation.\nGeneral way to compute inverse\nRow reduce (A | I_n) until you get I_n for A\nTherefore, if\n(E^n E^{n-1} ... E_1) A = I_n\nthen\n(E^n E^{n-1} ... E_1) = A^{-1}\nInvertible Matrix Theorem - Properties\nLet A be an n x n matrix. These statements are all equivalent\n\na) A is invertible.\nb) A is row equivalent to I^n.\nc) A has n pivotal columns. (All columns are pivotal.)\nd) Ax = 0 has only the trivial solution.\ne) The columns of A are linearly independent.\nf) The linear transformation x → Ax is one-to-one.\ng) The equation Ax = b has a solution for all b in R^n.\nh) The columns of A span R^n.\ni) The linear transformation x → Ax is onto.\nj) There is a n x n matrix C so that CA = I_n. (A has a left inverse.)\nk) There is a n x n matrix D so that AD = I_n. (A has a right inverse.)\nl) A^T is invertible.\n\nAbbreviated, invertible matrix theorem (IMT)\nAB = I \\implies A = B^{-1}, B = A^{-1}, B is invertible, A is invertible\nsquare A invertible \\iff\n\n0 not eigenvalue of A\ndet A \\not = 0\n\nSingular\nNoninvertible\nPartitioned/Block Matrix\nA partitioned matrix is a matrix that you write as a matrix of matrices\nWhen doing multiplication with a block matrix, make sure the “receiving” matrix’s entries go first, to respect the lack of commutativity in matrix multiplication. See HW 2.4 if this doesn’t make sense.\nRow Column Method\nLet A be m x n and B be n x p matrix. Then, the (i, j) entry of AB is\nrow_i A · col_j B.\nThis is the Row Column Method for matrix multiplication\nNotable HW Problem\n\nLU Factorization\nTriangular Matrices\nUpper Triangular: Nonzero along and above\nLower Triangular: Nonzero along and below\nIf A is an m x n matrix that can be row reduced to echelon form\nwithout row exchanges, then A = LU . L is a lower triangular m x m\nmatrix with 1’s on the diagonal, U is an echelon form of A.\nSuppose A can be row reduced to echelon form U without interchanging\nrows. Then,\nE_p ... E_0 A = U\nA = LU = (E_p ... E_0)^{-1}U\nTo compute the LU decomposition:\n\nReduce A to an echelon form U by a sequence of row replacement\noperations, if possible.\nPlace entries in L such that the same sequence of row operations\nreduces L to I.\n\nSubspaces of \\mathbb{R}^n\nSubset\nA subset of \\mathbb{R}^n, for example, is any collection of vectors that are in \\mathbb{R}^n\nSubspace\nA subset H of \\mathbb{R}^n is a subspace if it is closed within scalar multiplication and vector addition, i.e.\n\nc \\in \\mathbb{R}; \\vec{u}, \\vec{v} \\in H\nc \\vec{u} \\in H, \\vec{u} + \\vec{v} \\in H\n\\vec{0} \\in H\n\nColumnspace\nspan of columns of A\nsame as range of A\nNullspace\nspan of set of \\vec{x} that satisfy A\\vec{x} = \\vec{0}\nNull A = \\{\\vec{x} | A\\vec{x} = \\vec{0}\\}\nBasis\nLinearly independent vectors that span a subspace\nCoordinates, relative to a basis\nThere are many different possible choice of basis for a subspace. Our choice can give us dramatically different properties.\nStandard basis are i, j, k, but you can use other vectors to span the same amount of space if you want.\n\nWhat is a determinant? Given a linear transformation T, let us focus on the magnitude of the cross product of the basis vectors. The determinant would be the scalar factor between the original and transformed areas? (Yes)\nIf you are calculating some integral over a transformed space, is the jacobian just the determinant of the transformation, or is it related---possibly scaling the result to make sense given standard basis vectors? (Yes)\n\nDimension\nDimension/Cardinality of a non-zero subspace H, dim H, is the number of vectors in the basis of H. We define dim{0} = 0.\nTheorem\n\nAny two choices of \\mathcal{B}_1, \\mathcal{B}_2 of a non-zero subspace H have the same dimension*\n\nEx Problems\n\ndim \\mathbb{R}^n\n\nn\n\n\nH = \\{(x_1 ...., x_n) : x_1 + ... + x_n = 0\\} has dimension\n\nn - 1\nuse the idea of # 3\nn variables, solve for x_1 ito everything else. → one pivot everything else free vars. Therefore n - 1 free vars\n\n\ndim(Null A) is the number of\n\nnumber of free vars\n\n\ndim(Col A) is the number of\n\nnumber of pivots\n\n\n\nRank\nthe rank of a matrix A is the dimension of its column space\nnumber of pivots\nNullity\ndim(Null A) = Nullity\nnumber of of free vars\nNotation from class\n\nLet \\mathcal{B} \\in H\n\n\\mathcal{B} = \\{\\vec{b_1}, ..., \\vec{b_n}\\}\n\\mathcal{B} is some basis for the subspace H\n\n\n\n\\displaylines{\n\\vec{x} \\in H \\implies \\\\\n\\text{coords of $\\vec{x}$ relative to $\\mathcal{B}$ are $c_1, . . . , c_n$}\\quad \\vec{x} = c_1 \\vec{b_1} + ... + c_n \\vec{b_n}\\quad\\\\ \\\\ \\land \\\\\n\\text{coord vector of } \\vec{x} \\text{ relative to } \\mathcal{B}\\quad [\\vec{x}]_{\\mathcal{B}} = \\begin{bmatrix}\nc_1 \\\\\n... \\\\\nc_n\n\\end{bmatrix}\n}\n\nRank-Nullity Theorem \\star\nIf a matrix A has n columns, then\n\nRank(A) + Nullity(A) = n\ndim(Col(A)) + dim(Nul(A)) = n\n\nBasis Theorem\n\nAny two bases for a subspace have the same dimension\n\nInvertibility Theorem\n\nLet A be a n x n matrix. These conditions are equivalent.\n\nA is invertible\nThe columns of A are a basis for \\mathbb{R}^n\nCol A = \\mathbb{R}^n\nrank A = dim Col A = n\nNull A = {0}\n\n\nDeterminant\nImagine the area of parallelogram created by the basis of a standard vector space, like \\mathbb{R}^2. Now apply a linear transformation A to that vector space. The new area of the new parallelogram has been scaled by a factor of the determinant.\nS is the parallelopiped.\narea(T(S)) = |det(A)| \\cdot area(S)\nYou can also just think of it as the area of the parallelogram spanned by the columns of a matrix\nR^3 and beyond → parallelopiped and volume\n#star\n(assume n by n matrix because we only know how to find determinants for square matrices)\nYou can also get the area of S by using the determinant of the matrix created by the vectors that span S, i.e.\n|\\vec{a} \\times \\vec{b}| = area(S) \\implies |det([\\vec{a}\\ \\ \\vec{b}])| = area(S)\nbecause you are shifting the standard basis vectors into the vector space dictated by S\nDeterminant Laws\n\ndet(A) = 0 \\iff A is singular\n\ndet(A) \\not = 0 \\iff A is invertible\n\n\ndet(Triangular) = product of diagonals\ndet A = det A^T\ndet(AB) = det A · det B\ndet(A^{-1}) = \\frac{1}{det(A)}\ndet(kA) = k^n det(A)\n\nDeterminant Post Row Operations\nif A square:\n\nif adding rows to rows on A to get B then det A = det B\nif swapping rows in A to get B then -det A = det B\nif scaling one row of A by k, then k \\cdot det(A) = det(B)\nExactly the same for columns\n\nCofactor expansion\nWhat the diagonal 3x3 is shorthand for\nCofactor of an n x n matrix A is C_{ij} = (-1)^{i+j} det A_{ij}\n\\begin{bmatrix}\n+ &amp; - &amp; + &amp; ... \\\\\n- &amp; + &amp; - &amp; ... \\\\\n+ &amp; - &amp; + &amp; ... \\\\\n... &amp; ... &amp; ... &amp; ...\n\\end{bmatrix}\ndet A = a_{1j}C_{1j} + ... + a_{nj} C_{nj}\nFor +/- use pattern of current matrix in Q, not the og\nEigenvectors, Eigenvalues, and Eigenspaces\nEigenvectors and Eigenvalues\nGiven\n\nA is square\nA\\vec{v} defined, e.g. if A \\in \\mathbb{R}^{n\\times n} then \\vec{v} \\in \\mathbb{R}^n\n\nA\\vec{v} = \\lambda\\vec{v}\n\n\\vec{v} is an eigenvector for A\n\\lambda is the corresponding eigenvalue (\\lambda\\in \\mathbb{C})\nAn eigenvector is a vector solution \\vec{v} to the above equation, such that the linear transformation of A has the same result as scaling the vector by \\lambda.\n\nFurthermore:\nA\\vec{v} = \\lambda \\vec{v}\nA\\vec{v} - \\lambda \\vec{v}= 0\nA\\vec{v} - \\lambda \\vec{v}= 0\nA\\vec{v}-\\lambda I(\\vec{v})=0\n(A-\\lambda I)\\vec{v}=0\nYou can solve for \\lambda using in augmented matrix with this form\nNotes:\n\n\\lambda &gt; 0 \\implies A\\vec{v}, \\vec{v} point same direction\n\\lambda &lt; 0 \\implies A\\vec{v}, \\vec{v} point opposite direction\n\\lambda can be complex even if nothing else in the equation is\nEigenvalues cannot be determined from the reduced version of a matrixstar\n\ni.e. row reductions change the eigenvalues of a matrix\n\n\nThe diagonal elements of a triangular matrix are its eigenvalues.\nA invertible iff 0 is not an eigenvalue of A.\nStochastic matrices have an eigenvalue equal to 1.\nIf \\vec{v}_1 , \\vec{v}_2, . . . , \\vec{v}_k are eigenvectors that correspond to distinct eigenvalues, then \\vec{v}_1 , \\vec{v}_2, . . . , \\vec{v}_k are linearly independent\n\nEigenspaces\n\nthe span of the eigenvectors that correspond to a particular eigenvalue\n\n\nNul(A-\\lambda I)\n\nCharacteristic Equation\ndet(A-\\lambda I) = 0 to get values for \\lambda. Recall det(T)=0 means noninvertible. If a matrix isn’t invertible, then we won’t get trivial solutions when solving. Also the idea of reducing the dimension through the transformation is relevant; squishing the basis vectors all onto the same span where the area/volume is 0. Recall eigenvectors remain on the same span despite a linear transformation.\n\\lambda is an eigenvalue of A \\iff (A-\\lambda I) is singular\ntrace of a Matrix tr(M) is the sum of diagonal\nCharacteristic Polynomial\ndet(A-\\lambda I)\nn degree polynomial → n roots → maximum n eigenvalues (could be repeated)\n\nAlgebraic Multiplicity\nAlgebraic multiplicity of an eigenvalue is how many times an eigenvalue repeatedly occurs as the root of the characteristic polynomial.\nGeometric Multiplicity\n\nGeometric multiplicity of an eigenvalue is the number of eigenvectors associated with an eigenvalue; dim(Nul(A-\\lambda I)), which is saying how many eigenvector solutions does this eigenvalue have (recall dim(B) is number of free vars in B)\n\nSimilarity\n\nsquare A,B are similar \\iff we can find P so that A = PBP^{-1}\nA,B similar \\implies same characteristic polynomial \\implies same eigenvalues\n\nMarkov chains\nStochastic matrix\n\nMatrix that uses the rates/probabilities\nColumns are probability vectors.\nSum to 1\n\n\nProbability Vector\nSome vector \\vec{x} with nonnegative elements that sum to 1\nStochastic Matrix\nA stochastic matrix is a square matrix, P , whose columns are\nprobability vectors.\n|det(P)| ⇐ 1, only volume contracting or preserving\nMarkov Chain\nA Markov chain is a sequence of probability vectors, and a\nstochastic matrix P , such that:\n\\displaylines{\nP^k \\vec{x_0} = \\vec{x}_k\\\\\n\\vec{x}_{k+1} = P \\vec{x}_k ; k = 0, 1, 2, . . .\n}\nConvergence\nRegularity\n\nStochastic matrix is regular if there  \\exists (k \\geq 1) P^k strictly has positive entries\nRegular \\iff unique steady state vectors\n\nIrregular \\iff 0\\leq n\\not = 1 steady state vectors\n\n\n\nSteady-State Vector\nA steady-state vector for P is a vector \\vec{q} such that P \\vec{q} = \\vec{q}.\n(P-I)\\vec{q} = 0\nFixed point, I/O the same\nEx:\nDetermine the steady state vector for\nP = \\begin{bmatrix}\n.8 &amp; .3 \\\\\n.2 &amp; .7\n\\end{bmatrix}\nGoal: solve P\\vec{q} = \\vec{q}\n(P-I)\\vec{q} = \\vec{q}\n\\displaylines{\n\\begin{bmatrix}\n.8-1 &amp; .3 &amp; 0 \\\\\n.2 &amp; .7-1 &amp; 0\n\\end{bmatrix}\\\\\n\\begin{bmatrix}\n-.2 &amp; .3 &amp; 0 \\\\\n.2 &amp; -.3 &amp; 0\n\\end{bmatrix}\\\\\n\\begin{bmatrix}\n1 &amp; -\\frac{3}{2} &amp; 0 \\\\\n0 &amp; 0 &amp; 0\n\\end{bmatrix}\n}\n\\therefore \\vec{q} = t\\begin{bmatrix}\n\\frac{3}{2} \\\\\n1\n\\end{bmatrix}\n\\frac{3}{2}t + t = 1\n\\therefore t = \\begin{bmatrix}\n\\frac{3}{5} \\\\\n\\frac{2}{5}\n\\end{bmatrix}\nRelated to eigenvectors. \\vec{q} is defined as lim_{k\\rightarrow \\infty} \\left(P^k \\vec{x_0}\\right) = \\vec{q}, also P\\vec{q} = \\vec{q}\nWhen you reapply a linear transformation approaching infinity times, all the points in the subspace will approach the span of\n\nIf the transformation is regular, a single eigenvector\n\nFor our regular stochastic matrices, this is what the steady state vector is.\n\n\nIf the transformation is irregular, possibly multiple eigenvectors or none at all. If multiple, points will converge to the closest possible eigenspace.\n\nTheorem\n\nas k → \\infty\n\\vec{x}_{k+1} = P \\vec{x}_k ; k = 0, 1, 2, . . .\nIf P is a regular stochastic matrix, then P has a unique steady-state vector \\vec{q}, and \\vec{x_{k+1}} = P\\vec{x_k} converges to \\vec{q} as k \\rightarrow \\infty; (P^k \\vec{x_0} \\longrightarrow_{k\\rightarrow \\infty} \\vec{q}) where P\\vec{q} = \\vec{q}\n\nM3\nDiagonalization\nDiagonalizable\nComplex Eigenvalues\nComplex numbers\nConjugate \\overline{a + bi} = a - bi\n\nReflects across the Re(z) axis\nMagnitude (or “Modulus”) |a + bi| = \\sqrt{a^2 + b^2} = \\sqrt{(a+bi)(a-bi)}\nPolar a+ib = r(cos\\phi + i\\ sin\\phi) where r is the magnitude\n\nif x and y \\in \\mathbb{C}, \\vec{v} \\in \\mathbb{C}^n\n\n\\overline{(x+y)} = \\overline{x} + \\overline{y}\n\\overline{A\\vec{v}} = A \\overline{\\vec{v}}\nIm(x\\overline{x}) = 0\n\\overline{(xy)} = \\overline{x} \\overline{y}\n\nEuler’s Formula\nz = |z|e^{i\\phi}\nSuppose z_1 has an angle \\phi_1 and z_2 has \\phi_2\nThe product z_1 z_2 has angle \\phi_1 + \\phi_2, and modulus |z_1||z_2|\nCan use Euler’s formula to make it easier\nz_1 z_2 = (|z_1|e^{i\\phi_1})(|z_2|e^{i\\phi_2}) = (|z_1||z_2|e^{i(\\phi_1 + \\phi_2)})\nComplex Eigenvalues\nTheorem: Fundamental Theorem of Algebra\nAn n degree polynomial has exactly n complex roots (including multiplicity).\nTheorem\n\nIf \\lambda \\in \\mathbb{C} is a root of a real polynomial, \\overline{\\lambda} is also\n\nComplex roots come in complex conjugate pairs\n\n\nIf \\lambda is an eigenvalue of real matrix A, with eigenvector \\vec{v}, then \\overline{\\lambda} is an eigenvalue of A with eigenvector \\overline{\\vec{v}}\n\nExample\n4 of the eigenvalues of a 7 x 7 matrix are -2, 4 + i, -4 - i, and i\n\nBecause there are 3 nonconjugate complex pairs, we know that the remaining eigenvalues are the conjugates of the given complex values\nWhat is the characteristic polynomial?\np(\\lambda) = (\\lambda + 2)(\\lambda - (4+i))(\\lambda - (-4-i))(\\lambda - i) (\\lambda-(4-i))(\\lambda - (-4 + i))(\\lambda + i)\n\nExample\nThe matrix that rotates vectors by \\frac{\\pi}{4} radians about the origin and then scales vectors by \\sqrt{2} is:\n\\begin{bmatrix}\n1 &amp; -1 \\\\\n1 &amp; 1\n\\end{bmatrix}\nWhat are the eigenvalues of A? Find an eigenvector for each eigenvalue\ndet(A - \\lambda I) = (1 - \\lambda)^2 = \\lambda^2 - 2\\lambda +2\n\\lambda = \\frac{2 \\pm \\sqrt{4 - 8}}{2} = \\frac{2 \\pm 2i}{2} = 1 \\pm i\n\\lambda^{+}:\n\\displaylines{\nA - \\lambda^{+} I\\\\\n\\begin{bmatrix}\n1-(1+i) &amp; -1 \\\\\n1 &amp; 1-(1+i)\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n-i &amp; -1 \\\\\n1 &amp; -i\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n1 &amp; -i\\\\\n-i &amp; -1\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\n1 &amp; -i\\\\\n0 &amp; 0\n\\end{bmatrix} \\rightarrow \\vec{v_{+}} = \\begin{bmatrix}\ni \\\\\n1\n\\end{bmatrix}\n}\n\\lambda^{-}:\n\\displaylines{\n&quot;&quot;&quot;&quot;&quot;&quot;\\rightarrow \\vec{v_{-}} = \\begin{bmatrix}\n-i \\\\\n1\n\\end{bmatrix}\n}\nCould reason the eigenvalue for \\lambda^{-} by the fact that eigenvalues and their eigenvectors come in complex conjugate pairs.\nInner Product, Length, Orthogonality\nDot Product\n\n\\vec{u} \\cdot \\vec{v} = \\vec{u}^{T} \\vec{v}\n\\vec{u} \\cdot \\vec{v} = \\vec{v} \\cdot \\vec{u}\n(\\vec{u} + \\vec{v}) \\cdot \\vec{w} = \\vec{u} \\cdot \\vec{w} + \\vec{v} \\cdot \\vec{w}\n(c\\vec{u}) \\cdot \\vec{v} = c(\\vec{u} \\cdot \\vec{w})\n||c\\vec{v}|| = |c|\\ ||\\vec{v}||\n\\vec{u} \\cdot \\vec{u} \\geq 0\n\\vec{u} \\cdot \\vec{u} = ||\\vec{u}||^2\n\\vec{a} \\cdot \\vec{b} = |a||b|\\ cos\\theta\n\nLength\n||\\vec{u}|| = \\sqrt{\\vec{u} \\cdot \\vec{u}} = \\sqrt{u^{2}_{1} + \\cdots + u^{2}_n}\nOrthogonality\n\n||\\vec{u} + \\vec{w}||^2 = ||\\vec{u}||^2 + ||\\vec{w}||^2 \\lor \\vec{u} \\cdot \\vec{w} = 0 \\implies \\text{Orthogonal}\nIf W is a subspace of \\mathbb{R}^n and \\vec{z} \\in \\mathbb{R}^n, \\vec{z} is orthogonal to W if it is orthogonal to every vector in W\nThe set of all vectors orthogonal to a subspace is a itself a subspace, called the Orthogonal Complement of W, W^{\\perp}, W perp\n\nW^{\\perp} = \\{\\vec{z} \\in \\mathbb{R}^n \\mid \\forall (\\vec{w} \\in W)\\ \\vec{z} \\cdot \\vec{w} = 0\\}\n\n\ndim(Row\\ A) = dim(Col\\ A)\n(Row\\ A)^{\\perp} = Nul\\ A\n(Col\\ A)^{\\perp} = Nul (A^T)\n\n\\vec{x} \\in Nul(A) \\iff\n\nA\\vec{x} = \\vec{0}\n\\vec{x} is orthogonal to each row of A\nRow\\ A is orthogonal complement to Nul\\ A\ndim(Row\\ A) + dim(Nul\\ A) = number of columns\n\n\\vec{a} \\cdot \\vec{b} = 0 \\land \\vec{a} \\not = \\vec{0}, \\vec{b} \\not = 0 \\iff \\vec{a} \\perp \\vec{b} \\iff a \\text{ and } b \\text{ are Orthogonal}\nOrthogonal Sets\nOrthogonal Vector Sets\nA set of vectors are an orthogonal set of vectors if every pair in the set is orthogonal to every other vector in the set.\nLinear Independence for Orthogonal Sets:\nIf there is an orthogonal set of vectors O, then\n||c_1 \\cdot \\vec{u}_1 + \\cdots  + c_n \\cdot \\vec{u}_n||^2 = c_1^2 \\cdot ||\\vec{u}_1||^2 + \\cdots + c_n^2 \\cdot ||\\vec{u}_n||^2\n\\vec{0} \\not \\in O \\implies O lin. indep.\nExpansion in Orthogonal Basis\nIf O is the basis for subspace W in \\mathbb{R}^n and O is an orthogonal basis,\nthen for any vector \\vec{w} \\in W\n\\vec{w} = c_1 \\vec{u}_1 + \\cdots + c_n \\vec{u}_n\nc_i=\\frac{\\vec{w} \\cdot \\vec{u}_i}{\\vec{u}_i \\cdot \\vec{u}_i}\nOrthogonal Projection\nOrthogonal Decomposition\nBest Approximation\nGram-Schmidt Process\nQR Factorization\nLeast Squares\nExam 4\nGoogle Page Rank\nSymmetric Matrix\nSpectral Decomposition\nOrthogonal Diagonalization\nQuadratic Form\nConstrained Optimization\nSingular Value Decomposition"},"Notes/Linear-Algebra/Negative-Definite":{"title":"Negative Definite","links":["Notes/Linear-Algebra/Negative-Definite"],"tags":[],"content":"\\forall \\vec{x}\\ x^T A x &lt; 0, then A is Negative Definite"},"Notes/Linear-Algebra/Negative-Semidefinite":{"title":"Negative Semidefinite","links":["Notes/Linear-Algebra/Negative-Semidefinite"],"tags":[],"content":"\\forall \\vec{x}\\ x^T A x \\geq 0, then A is Negative Semidefinite"},"Notes/Linear-Algebra/Normal":{"title":"Normal","links":[],"tags":[],"content":"For a vector, of unit magnitude."},"Notes/Linear-Algebra/Orthogonal-Basis":{"title":"Orthogonal Basis","links":["Notes/Linear-Algebra/Orthonormal-Basis"],"tags":[],"content":"A basis where each vector is orthogonal to to every other vector.\nLet W be a subspace of \\mathbb{R}^n and has an orthogonal basis formed by vectors: \\vec{u_1}, ... \\vec{u_p}\n\\forall (\\vec{u_i} \\in W), \\forall (\\vec{u_j} \\in W)\\ ((i \\not = j) \\implies (\\vec{u_i} \\cdot \\vec{u_j} = 0))\nSee Orthonormal Basis"},"Notes/Linear-Algebra/Orthogonal-Complement":{"title":"Orthogonal Complement","links":[],"tags":[],"content":"The orthogonal compliment to a subspace W is the the set of all vectors that are orthogonal to W.\ni.e.\nW^{\\perp} = \\forall(\\vec{y} \\in W)\\{\\vec{x} \\mid \\vec{x} \\cdot \\vec{y} = 0\\}"},"Notes/Linear-Algebra/Orthogonal-Decomposition":{"title":"Orthogonal Decomposition","links":["Notes/Linear-Algebra/Orthogonal-Complement","Notes/Linear-Algebra/Orthogonal-Basis","Notes/Linear-Algebra/Best-Approximation"],"tags":[],"content":"I proceed to define the orthogonal decomposition for some vector y, where y \\in \\mathbb{R}^n, where W is a subspace of \\mathbb{R}^n, where W^{\\perp} is the Orthogonal Complement of W, where \\hat{y} is the orthogonal projection of \\vec{y} onto W, and \\vec{z} is a vector orthogonal to \\hat{y}\ny = (\\hat{y} \\in W) + (\\vec{z} \\in W^{\\perp})\n\n\nEvery y \\in \\mathbb{R}^n has a unique sum in the form above, so long as W is a subspace of \\mathbb{R}^n\ndim(W) + dim(W^{\\perp}) = n\n\nConcerning \\hat{y}\nIf \\vec{u_1}, \\cdots, \\vec{u_p} is an Orthogonal Basis for W, then \\hat{y}, the orthogonal projection of \\vec{y} onto W is given by:\n\\displaylines{\n\\hat{y} =  proj_{W}\\vec{y} = proj_{\\vec{u_1}}\\vec{y} + \\cdots + proj_{\\vec{u_p}}\\vec{y} = proj_{W} \\vec{y{}}\n}\nSee Best Approximation, but in essence \\hat{y} is the closest vector in W to \\vec{y}\nExamples:\nHW 6.3 Q1\nHW 6.3 Q5\nHW 6.3 Q6"},"Notes/Linear-Algebra/Orthogonal-Diagonalization":{"title":"Orthogonal Diagonalization","links":["Notes/Linear-Algebra/Diagonalization"],"tags":[],"content":"Like Diagonalization, but you perform it on an orthogonal matrix, which makes the process easier as follows:\nS = PDP^T\nWhere P^{-1} = P^T, i.e. P is orthogonal\nIt is otherwise the same as performing diagonalization on an arbitrary matrix"},"Notes/Linear-Algebra/Orthogonal-Matrix":{"title":"Orthogonal Matrix","links":["Notes/Linear-Algebra/Orthonormal"],"tags":[],"content":"A matrix whose columns are Orthonormal\nU is orthogonal \\iff U^T U = I_n\nU \\square \\implies U^T = U^{-1}\nProperties\nPreserves Length\n||U\\vec{x}|| = ||\\vec{x}||\nPreserves Angles\n(U\\vec{x}) \\cdot (U\\vec{y}) = \\vec{x} \\cdot \\vec{y}\nPreserves Orthogonality\n(U\\vec{x}) \\cdot (U\\vec{y}) = \\vec{x} \\cdot \\vec{y} \\implies (\\vec{x} \\cdot \\vec{y} = 0 \\iff (U\\vec{x}) \\cdot (U\\vec{y}) = 0)"},"Notes/Linear-Algebra/Orthogonal-Projection":{"title":"Orthogonal Projection","links":[],"tags":[],"content":"\\displaylines{\n\\text{proj}_{\\vec{u}} \\vec{v} = \n\n\\left( \\vec{v} \\cdot \\hat{u} \\right)\\hat{u}\n\n=\n\\frac{\\vec{v} \\cdot \\vec{u}}{||\\vec{u}||}\\frac{\\vec{u}}{||\\vec{u}||}\n = \n\n\\frac{\\vec{v} \\cdot \\vec{u}}{\\vec{u} \\cdot \\vec{u}}\\vec{u}\\\\\n\n\\vec{w} = \\vec{v} - \\text{proj}_{\\vec{u}}\\vec{v}\\quad \\quad\n\\vec{v} = \\vec{w} + \\text{proj}_{\\vec{u}}\\vec{v}\\quad \\quad\n||\\vec{v}||^2 = ||\\vec{w}||^2 + ||\\text{proj}_{\\vec{u}}\\vec{v}||^2\n}"},"Notes/Linear-Algebra/Orthogonal":{"title":"Orthogonal","links":[],"tags":[],"content":"Perpendicular.\n\\vec{a} and \\vec{b} are orthogonal if \\vec{a} \\cdot \\vec{b} = 0"},"Notes/Linear-Algebra/Orthonormal-Basis":{"title":"Orthonormal Basis","links":["Notes/Linear-Algebra/Orthonormal","Notes/Linear-Algebra/Orthogonal","Notes/Linear-Algebra/Normal"],"tags":[],"content":"An orthonormal basis for a subspace W is an basis for which every vector is Orthonormal, i.e. Orthogonal and Normal\nIf \\{\\vec{u_1}, \\cdots, \\vec{u_p}\\} is an orthonormal basis for W:\n\\vec{w} = (\\vec{w} \\cdot \\vec{u_1})\\vec{u_1} + \\cdots + (\\vec{w} \\cdot \\vec{u_p})\\vec{u_p}\n||\\vec{w}|| = \\sqrt{(\\vec{w} \\cdot \\vec{u_1})^2 + \\cdots + (\\vec{w} \\cdot \\vec{u_p})^2}"},"Notes/Linear-Algebra/Orthonormal":{"title":"Orthonormal","links":[],"tags":[],"content":""},"Notes/Linear-Algebra/Positive-Definite":{"title":"Positive Definite","links":["Notes/Linear-Algebra/Positive-Definite"],"tags":[],"content":"\\forall \\vec{x}\\ x^T A x &gt; 0, then A is Positive Definite"},"Notes/Linear-Algebra/Positive-Semidefinite":{"title":"Positive Semidefinite","links":["Notes/Linear-Algebra/Positive-Semidefinite"],"tags":[],"content":"\\forall \\vec{x}\\ x^T A x \\geq 0, then A is Positive Semidefinite"},"Notes/Linear-Algebra/QR-Factorization":{"title":"QR Factorization","links":[],"tags":[],"content":"For any m \\times n matrix A, with linearly independent columns:\nA = QR\nQ is\n\nm x n\nits columns are an orthonormal basis for Col A\nR is\nn x n\nupper triangular\npositive diagonal\n||\\vec{r}_j|| = ||\\vec{a}_j||\\quad \\quad where \\vec{r} and \\vec{a} are columns of R and A\n\n"},"Notes/Linear-Algebra/Quadratic-Form":{"title":"Quadratic Form","links":["Notes/Linear-Algebra/Positive-Definite","Notes/Linear-Algebra/Positive-Semidefinite","Notes/Linear-Algebra/Negative-Definite","Notes/Linear-Algebra/Negative-Semidefinite"],"tags":[],"content":"\nChange of variable\nPositive Definite\nPositive Semidefinite\nNegative Definite\nNegative Semidefinite"},"Notes/Linear-Algebra/Singular-Value-Decomposition":{"title":"Singular Value Decomposition","links":["Notes/Linear-Algebra/Gram-Schmidt-Process"],"tags":[],"content":"www.youtube.com/watch\nApplications\n\nIf A is a invertible square matrix then the condition number is the largest singular value divided by the smallest singular value\n\nCondition number describes the sensitivity of a solution to Ax = b to errors in A\nA problem with a low condition number is said to be well-conditioned, while a problem with a high condition number is said to be ill-conditioned\nDescribe difficulty in computing inverse\nChaos: small change in input results in massive change in output\n\n\nCan use SVD to talk about rkA, ColA, RowA, NulA, (Col\\ A)^\\perp, etc.\n\n\nrkA = rk\\Sigma\n\nColA = U columns through dim A\n\nbc A\\vec{v} \\propto \\vec{u}\n\n\nCol A perp = U columns after dim A\n\nby def\n\n\nNul A = V columns that correspond to the free columns of U\n\n\n\nProcess\nA = U\\Sigma V^T\nU and V are square, guaranteed.\n\nSingular values: \\sqrt{\\text{eigenvalues of } A^T A}\nConstruct \\Sigma using the singular values. \\Sigma has the same shape as A, with a diagonal matrix of the singular values in the top left corner\nV = matrix of eigenvectors of A^T A\nCompute an orthonormal basis for Col A: use A\\vec{v_i} = \\sigma_i \\vec{u_i} for dim V\nAfterwhich, extend and fill up the remaining orthonormal basis\n\nOption A: Rawdog it-think about it, so to speak\nOption B: Gram-Schmidt Process\nOption C: Use Nul\\ AA^T\n\n\nConstruct the columns of U with the \\vec{u_i} vectors\nNote: for U you can also get it via the V process but with AA^T, for eigenvalue 0, find eigenvector\nV and U are orthogonal btw, and they have dimensions of A^T A and AA^T\n\nA = \\sum^{r}_{s=1} \\sigma_s \\vec{u}_s \\vec{v}^T_s\nwhere \\vec{u}_s, \\vec{v}_s are the s^{\\text{th}} columns of U and V"},"Notes/Linear-Algebra/Spectral-Decomposition":{"title":"Spectral Decomposition","links":[],"tags":[],"content":"www.youtube.com/watch\nSymmetric\nRecall: If P is an orthogonal n × n matrix, then P −1 = P T , which\nimplies A = PDP^T is diagonalizable and symmetric.\nSpectral Thm\nAn n × n symmetric matrix A has the following properties.\n\nAll eigenvalues of A are real.\nThe dimenison of each eigenspace is full, that it’s dimension is equal to it’s algebraic multiplicity.\nThe eigenspaces are mutually orthogonal.\nA can be diagonalized: A = PDP^T , where D is diagonal and P is orthogonal.\n\nSpectal Decomp\n"},"Notes/Linear-Algebra/Symmetric-Matrix":{"title":"Symmetric Matrix","links":[],"tags":[],"content":"Definition\nMatrix A is symmetric if A^T = A\nThe eigenspaces of a symmetric matrix are orthogonal"},"Notes/Physics/Ampère's-Law-With-Maxwell's-Addition":{"title":"Ampère's Law With Maxwell's Addition","links":["Notes/Physics/Ampère's-Law","tags/todo"],"tags":["todo"],"content":"A complete form of Ampère’s Law.\n\\nabla \\times B = \\mu_{0} (J + \\epsilon_{0} \\frac{\\partial E}{\\partial t})\n\\oint_{C} \\vec{B} \\cdot d \\vec{l} = \\mu_{0} (I + \\epsilon_{0} \\frac{d \\Phi_{E}}{d t})\nConnection between differential and integral form\ntodo"},"Notes/Physics/Ampère's-Law":{"title":"Ampère's Law","links":["Notes/Physics/Ampère's-Law-With-Maxwell's-Addition","Notes/Physics/Common-Cases-of-Ampère's-Law"],"tags":[],"content":"The closed loop line integral of a magnetic field is proportional to the current the loop encloses. Right hand rule dictates the sign of the integral. Over a stationary path.\nIt is an incomplete model without Ampère’s Law With Maxwell’s Addition\n\\oint \\vec{B} \\cdot d \\vec{l} = \\mu_{0}I_{enclosed}\nThere are a variety of Common Cases of Ampère’s Law."},"Notes/Physics/Biot-Savart-Law":{"title":"Biot-Savart Law","links":["Notes/Physics/Electromagnetism"],"tags":[],"content":"A fundamental aspect of the concept of Electromagnetism is that a moving electric field creates a magnetic field. The direction of a field follows the right hand rule, with the velocity being the thumbs and the fingers being the magnetic field. B \\propto I.\nPoint Charge\nIn terms of a single moving charge, the magnetic field is as follows:\n\\vec{B} = \\frac{\\mu_{0}}{4 \\pi} \\frac{q}{r^{2}} \\vec{v} \\times \\hat{r}\nLive Wire\nd\\vec{B} = \\frac{\\mu_{0}}{4 \\pi} \\frac{I}{r^{2}} d\\vec{l} \\times \\hat{r}\nIf the wire is infinitely long and straight\nB = \\frac{\\mu_{0}I}{2 \\pi r}"},"Notes/Physics/Common-Cases-of-Ampère's-Law":{"title":"Common Cases of Ampère's Law","links":["Notes/Physics/Solenoid"],"tags":[],"content":"Distance r away from an infinitely long wire\nB = \\frac{\\mu_{0}I}{2 \\pi r}\nInside a Solenoid with n turns per unit length\nB = \\mu_{0}nIInside a toroidal Solenoid with total N turns\nB = \\frac{\\mu_{0}NI}{2 \\pi r}"},"Notes/Physics/EMF":{"title":"EMF","links":[],"tags":[],"content":"Electromotive force (EMF) is an energy transfer to an electric circuit per unit of electric charge, measured in volts.\nIt is not a force, and it would be more accurate to call it source voltage."},"Notes/Physics/Electromagnetic-Waves":{"title":"Electromagnetic Waves","links":["Notes/Physics/Speed-of-Light","Notes/Physics/Poynting-Vector"],"tags":[],"content":"Electromagnetic waves propagate at the Speed of Light. The direction of the radiation or “power flow” corresponds to the Poynting Vector, which is proportional to the cross product of the electric and magnetic fields of the radiation.\n"},"Notes/Physics/Electromagnetism":{"title":"Electromagnetism","links":["Notes/Physics/Maxwell's-Equations","Notes/Physics/Biot-Savart-Law","Notes/Physics/Faraday's-Law"],"tags":[],"content":"The concept relating electric and magnetic fields together into a singular entity.\nCan be described in terms of Maxwell’s Equations.\nMoving electric fields create magnetic fields (like in the Biot-Savart Law) and changing magnetic fields induce electric fields (like in Faraday’s Law)."},"Notes/Physics/Energy-in-Driven-Circuit-Elements":{"title":"Energy in Driven Circuit Elements","links":["tags/todo"],"tags":["todo"],"content":"todo"},"Notes/Physics/Faraday's-Law":{"title":"Faraday's Law","links":["Notes/Physics/EMF","Notes/Physics/Flux","Notes/Physics/Solenoid"],"tags":[],"content":"The EMF around a closed path is equal to the negative of the time rate of change of the magnetic Flux enclosed by the path.\n\\mathcal{E} = -\\frac{d \\Phi_B}{dt}\nAccordingly, a Solenoid with N total turns will have an EMF N times as large as that induced by just one of its loops:\n\\mathcal{E} = -N\\frac{d \\Phi_B}{dt}\nOf a rotating loop with N total turns and angular velocity \\omega:\n\\mathcal{E} = -N\\frac{d \\Phi_{B}}{dt} = NBA \\omega sin(\\omega t)\nStationary Faraday’s Law\nIf the path is stationary:\n\\oint \\vec{E} \\cdot d \\vec{l} = -\\frac{d \\Phi_B}{dt}\nInside a Solenoid:\n\\oint \\vec{E} \\cdot d \\vec{l} = -\\frac{dB}{dt}A = -\\mu_{0}n \\frac{dI}{dt} A"},"Notes/Physics/Flux":{"title":"Flux","links":[],"tags":[],"content":"The result of a surface integral, e.g.\n\\Phi = \\iint \\vec{F} \\cdot d \\vec{S}\nPerpendicular-flowiness of the vector field through the surface."},"Notes/Physics/Gauss'-Law-for-Electricity":{"title":"Gauss' Law for Electricity","links":["Notes/Physics/Flux","tags/todo"],"tags":["todo"],"content":"The total electrostatic Flux through a closed surface S is proportional to the charge enclosed by that surface.\n\\nabla \\cdot E = \\frac{\\rho}{\\epsilon_0}\n\\oiint_{S} \\vec{E} \\cdot d \\vec{S} = \\frac{Q_{enclosed}}{\\epsilon_0}\nConnection between differential and integral form\ntodo"},"Notes/Physics/Gauss'-Law-for-Magnetism":{"title":"Gauss' Law for Magnetism","links":["Notes/Physics/Flux","tags/todo"],"tags":["todo"],"content":"The total magnetic Flux through a closed surface S is equal to 0.\n\\nabla \\cdot B = 0\n\\oiint_{S} \\vec{B} \\cdot d \\vec{S} = 0\nConnection between differential and integral form\ntodo"},"Notes/Physics/Impedence":{"title":"Impedence","links":["tags/todo"],"tags":["todo"],"content":"todo"},"Notes/Physics/Inductance":{"title":"Inductance","links":["inductor","Notes/Physics/Solenoid"],"tags":[],"content":"The angular inertia of the flywheel shit. L is inductance.\nL = \\frac{\\Phi(i)}{i}\nInductance remains constant for a given inductor so long as its geometry doesn’t change due to B(i) \\propto i\nL = \\frac{\\Phi(i)}{i}\nL = \\frac{\\Phi(i)}{i}=\\frac{B(i)\\cdot A}{i}$ = \\frac{(C \\cdot i) \\cdot A}{i} = CA\nAccordingly, if you have a Solenoid with N turns and \\Phi is the flux through a single turn.\nL = N\\frac{\\Phi(i)}{i}"},"Notes/Physics/Inductor":{"title":"Inductor","links":["Notes/Physics/Solenoid","Notes/Physics/Inductance"],"tags":[],"content":"A Solenoid. Inductors resist change in current. Inductors have a property called Inductance.\nA good analogy for an inductor is a flywheel, where angular inertia represents resistance to current change.\nInductance\nL = \\frac{\\Phi(i)}{i}\nInductance is constant so long as the given inductor’s geometry doesn’t change, as explained in Inductance.\nVoltage\n\\Delta V_{L}=L \\frac{di}{dt}"},"Notes/Physics/LC-Series-DC-Circuit":{"title":"LC Series DC Circuit","links":["tags/todo"],"tags":["todo"],"content":"todo"},"Notes/Physics/LRC-Series-AC-Circuit":{"title":"LRC Series AC Circuit","links":["tags/todo"],"tags":["todo"],"content":"todo"},"Notes/Physics/LRC-Series-DC-Circuit":{"title":"LRC Series DC Circuit","links":["tags/todo"],"tags":["todo"],"content":"todo"},"Notes/Physics/Maxwell's-Equations":{"title":"Maxwell's Equations","links":["Notes/Physics/Gauss'-Law-for-Electricity","Notes/Physics/Maxwell-Faraday-Equation","Notes/Physics/Gauss'-Law-for-Magnetism","Notes/Physics/Ampère's-Law-With-Maxwell's-Addition"],"tags":[],"content":"Created by Maxwell.\nMicroscopic\nFor vacuum.\n\\nabla \\cdot E = \\frac{\\rho}{\\epsilon_0}\nGauss’ Law for Electricity\n\\oiint_{S} \\vec{E} \\cdot d \\vec{S} = \\frac{Q_{enclosed}}{\\epsilon_0}\n\\nabla \\times E = - \\frac{\\partial B}{\\partial t}\nMaxwell-Faraday Equation\n\\mathcal{E} = \\oint \\vec{E} \\cdot d \\vec{l} = -\\frac{d \\Phi_{B}}{d t} = - \\frac{d}{dt} \\oiint_{S} \\vec{B} \\cdot d \\vec{S} = - \\oiint_{S} \\frac{\\partial B}{\\partial t} \\cdot d \\vec{S}\n\\nabla \\cdot B = 0\nGauss’ Law for Magnetism\n\\oiint_{S} \\vec{B} \\cdot d \\vec{S} = 0\n\\nabla \\times B = \\mu_{0} (J + \\epsilon_{0} \\frac{\\partial E}{\\partial t})\nAmpère’s Law With Maxwell’s Addition\n\\oint_{C} \\vec{B} \\cdot d \\vec{l} = \\mu_{0} \\left(I + \\epsilon_{0} \\frac{d \\Phi_{E}}{d t}\\right)= \\mu_{0} (\\oiint_{S} \\vec{J} \\cdot d \\vec{S} + \\epsilon_{0} \\frac{d}{dt} \\oiint_{S} \\vec{E} \\cdot d \\vec{S})\nMacroscopic\nFor a medium. You get the integral forms the same way.\n\\nabla \\cdot D = \\rho\n\\nabla \\times E = - \\frac{\\partial B}{\\partial t}\n\\nabla \\cdot B = 0\n\\nabla \\times H = J + \\frac{\\partial D}{\\partial t}"},"Notes/Physics/Maxwell-Faraday-Equation":{"title":"Maxwell-Faraday Equation","links":["Notes/Physics/Maxwell's-Equations","Notes/Physics/Faraday's-Law","tags/todo"],"tags":["todo"],"content":"The differential form of this particular one of Maxwell’s Equations.\n\\nabla \\times E = -\\frac{\\partial B}{\\partial t}\n\\mathcal{E} = \\oint \\vec{E} \\cdot d \\vec{l} = -\\frac{d \\Phi_{B}}{d t} = - \\frac{d}{dt} \\oiint_{S} \\vec{B} \\cdot d \\vec{S} = - \\oiint_{S} \\frac{\\partial B}{\\partial t} \\cdot d \\vec{S}\nAlso: Faraday’s Law.\nConnection between differential and integral form\ntodo"},"Notes/Physics/Poynting-Vector":{"title":"Poynting Vector","links":[],"tags":[],"content":"\\vec{S} is the Poynting vector, which corresponds to the direction of the electromagnetic wave or its power flow.\n\\vec{S} = \\frac{1}{\\mu_{0}} \\vec{E} \\times \\vec{B}\nWhere I is intensity in \\frac{W}{m^2}\nI = S_{avg} = \\frac{E_{max}B_{max}}{2 \\mu_{0}}= \\frac{E_{max}^2}{2 \\mu_{0}c}\nE = cB\n"},"Notes/Physics/RL-Series-DC-Circuit":{"title":"RL Series DC Circuit","links":["Notes/Physics/EMF"],"tags":[],"content":"Where \\mathcal{E} is the EMF of the battery that charges the circuit.\nGrowth\ni(t)=\\frac{\\mathcal{E}}{R}(1-e^{-\\frac{R}{L}t})\nDecay\ni(t)=\\frac{\\mathcal{E}}{R}e^{-\\frac{R}{L}t}"},"Notes/Physics/Reactance":{"title":"Reactance","links":["tags/todo"],"tags":["todo"],"content":"todo"},"Notes/Physics/Solenoid":{"title":"Solenoid","links":[],"tags":[],"content":"A wire loop with turns in it. You get a magnetic field through one when current goes through it such that B \\propto I.\nB =\\mu_0nI"},"Notes/Physics/Speed-of-Light":{"title":"Speed of Light","links":["Notes/Physics/Maxwell's-Equations","tags/todo"],"tags":["todo"],"content":"c = \\frac{1}{\\sqrt{\\mu_{0}\\epsilon_0}}\nDerivation using Maxwell’s Equations\ntodo"},"index":{"title":"Home","links":["media/ResumeMilesKent.pdf"],"tags":[],"content":"Hey there. Feel free to take a look around.\n\nResume\nGitHub\n"}}